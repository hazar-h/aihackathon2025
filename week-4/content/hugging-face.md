Week 4 â€” Hugging Face & Model Deployment

This week focuses on Hugging Face, a powerful open platform for machine learning models, datasets, and AI pipelines. Youâ€™ll learn how to use Hugging Face models in Google Colab, interact with them via APIs, and explore model fine-tuning and inference.

What is Transformers?

Transformers is an open-source Python library developed by Hugging Face that provides pretrained deep learning models for Natural Language Processing (NLP), Computer Vision (CV), Speech, and Multimodal tasks all under a unified and simple API.

What the datasets library is:

The datasets library by Hugging Face is an open-source toolkit that provides: Access to thousands of pre-loaded datasets (text, image, audio, etc.) Tools to load, filter, split, and preprocess data easily A very efficient backend (uses Apache Arrow under the hood) for fast data access even with huge datasets

You can explore datasets here ðŸ‘‰ https://huggingface.co/datasets

ðŸŽ¯ Goal for the Week

Understand Hugging Face ecosystem and model repositories Explore and run Hugging Face models via Google Colab

ðŸ“š Courses & Notebooks (https://www.deeplearning.ai/short-courses/open-source-models-hugging-face/)

ðŸ’» How to Use Hugging Face on Google Colab

Open the Notebook Use the provided Colab links above or upload your .ipynb file to Google Colab. https://colab.research.google.com/drive/1CXk3ROKjdYtTlk4oGsJv1RL3pEMbhk-y?usp=sharing https://colab.research.google.com/drive/1qJP_RYglPNx8JljXika3KOlKHqUxY7NJ?usp=sharing